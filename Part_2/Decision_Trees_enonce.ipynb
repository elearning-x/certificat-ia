{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9ZJJLDDDdrz"
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4PDe4X-Dkrr"
   },
   "source": [
    "## 1 - Understanding the influence of hyperparameters - Simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE6QpWmMHQ6d"
   },
   "source": [
    "We want to study the influence of the different hyperparameters of a decision tree on its predictive performance. \n",
    "\n",
    "We first use the following lines to generate and plot our dataset, containing two input variables and one binary output. We are well aware that this is nothing like a real dataset. However, it is often better to look at a method applied on synthetic data to understand its behavior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "nUFyENIkGTbi",
    "outputId": "0adf03bb-ef24-42c0-8570-8b0860f0fe05"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate a classification dataset composed of two circles\n",
    "#Generating a training set\n",
    "X_train, y_train = make_circles(n_samples=200, noise=0.17)\n",
    "#Generating a test set\n",
    "X_test, y_test = make_circles(n_samples=10000, noise=0.17)\n",
    "\n",
    "# Plot the generated dataset\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d97BBSAIHm8B"
   },
   "source": [
    "**1) Using 'DecisionTreeClassifier', fit a decision tree with the default parameters on the training set. Compute its accuracy on the training set and test set. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBua6UOzIDC3",
    "outputId": "3e2582a7-45a6-48e8-b8f2-3657a5c6a52a"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kinNUshmKAQC"
   },
   "source": [
    "**2) You can plot the decision frontier with the following lines. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "G01lYhvnI1oa",
    "outputId": "9217334b-f5df-4c1a-da14-0020a4af0ecb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the range for the plot\n",
    "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "\n",
    "# Create a grid of points and classify each point\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='bwr')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary of Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMQUIegiK9ZQ"
   },
   "source": [
    "**3) We say that our algorithm overfits, as the performance on the training set is way better than that on the test set. By looking at the function 'DecisionTreeClassifier', what parameters could we change to decrease this phenomenon?**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zERHXEcNLSL6"
   },
   "source": [
    "####### To Be Completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_LbER_kMkgb"
   },
   "source": [
    "**4) Tuning of the depth. Train a tree of depth $15$. Between this tree and the first tree you trained, which one do you prefer?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jniCsLu1J8ZY",
    "outputId": "df791543-ad48-4157-8c3e-26eb05463fa4"
   },
   "outputs": [],
   "source": [
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-gz93aCOVc0"
   },
   "source": [
    "**5) Now we want to find the optimal depth. To this aim, plot the test error as a function of the tree depth. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ytuFuNciNbx1",
    "outputId": "1bbf625e-e98f-4138-b31f-abae822a4f86"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "#Setting the maximal depth to 15 according to the previous question\n",
    "max_depth = 15\n",
    "depths = np.arange(1, max_depth+1)\n",
    "errors_train = []\n",
    "errors_test = []\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRbxw0hnUcna"
   },
   "source": [
    "**6) Now we want to find the optimal value for the parameter max-leaf-nodes. To this aim, plot the test error as a function of the number of leaves. Comment.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "iBM15CF-Osi7",
    "outputId": "deaf2d1b-e433-4741-8e6a-7a217aaeed2e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "#Setting the maximal depth to 15 according to the previous question\n",
    "max_leaves = 200\n",
    "leaves = np.arange(2, max_leaves+1, 10)\n",
    "errors_train = []\n",
    "errors_test = []\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j__0wSMUX77J"
   },
   "source": [
    "**7) Finally, we want to study the impact of pruning. To this aim, plot the test error as a function of the complexity parameter 'ccp_alpha'. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "lVL_lNHSYFub",
    "outputId": "a3156818-e086-4f7b-8381-face99f9f7df"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "#Setting the maximal depth to 15 according to the previous question\n",
    "ccp = np.arange(0, 0.035, 0.001)\n",
    "errors_train = []\n",
    "errors_test = []\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vTRnmNpZjE4"
   },
   "source": [
    "## 2 - Applying Decision Trees on a real data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9elcg-UZmPf"
   },
   "source": [
    "Now, we want to apply a decision tree to solve a real problem. Let us consider the following real estate data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CReeyXR_Z4CY"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "house = fetch_california_housing()\n",
    "X, y = house.data, house.target\n",
    "feature_names = house.feature_names\n",
    "\n",
    "feature_mapping = {\n",
    "    \"MedInc\": \"Median income in block\",\n",
    "    \"HousAge\": \"Median house age in block\",\n",
    "    \"AveRooms\": \"Average number of rooms\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms\",\n",
    "    \"Population\": \"Block population\",\n",
    "    \"AveOccup\": \"Average house occupancy\",\n",
    "    \"Latitude\": \"House block latitude\",\n",
    "    \"Longitude\": \"House block longitude\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC7YJ4iDaHEa"
   },
   "source": [
    "**8) Use the command 'train_test_split' to split the data set into a training set and test set. The test set will only be used to assess the performance of our final estimator.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgHvVuZ7aAkn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZZE35T4DMm2"
   },
   "source": [
    "**9) Train a decision tree on the above data using default parameters. Evaluate its quadratic risk on the training set and on the test set. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4biSpfjDTyQ",
    "outputId": "c62bd92e-391a-4e92-ff5c-a397e3238df1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjbyvfWFbXls"
   },
   "source": [
    "**10) Tune the complexity parameter of the pruning by cross-validation by evaluating parameter values between 0.001 and 0.03. You can make use of the command 'cross_val_score'. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "7CXIanYmabge",
    "outputId": "54637c03-8cfb-49dd-fb42-1c33aa09f000"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "w9TeiReRb3jh",
    "outputId": "471ba12f-e6e3-489a-d293-c4a7e4a57dfb"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R72dbO8_kTVq"
   },
   "source": [
    "**11) Based on the previous question, find the pruning parameter that leads to the best predictive performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxeN73fQBh72",
    "outputId": "e5ba0849-8e75-48e0-95e8-f10b5e8dc938"
   },
   "outputs": [],
   "source": [
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbNVRD62k0NM"
   },
   "source": [
    "**12) Train a tree on the whole training set with the best pruning complexity (the one determined above) and evaluate its performance on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdybqQmyCfg5",
    "outputId": "4138be75-92a4-4160-b7c3-18bbfb421787"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# TO DO \n",
    "\n",
    "# END TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlwUlnEbk0g7"
   },
   "source": [
    "**13) Plot the first level of the tree. Comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "q2mrAy1Xjtyb",
    "outputId": "6653828d-124a-4edf-f2d8-99c81fa96be6"
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, max_depth=3, feature_names=house.feature_names,  filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
